version: 1
name: "self"

vars:
  project_name: "self"
  tts_server: "http://curiosity:7860"
  tti_server: "http://192.168.1.64:8570"
  itv_server: "http://192.168.1.64:8100"
  ttv_server: "http://192.168.1.64:6000"
  ollama_server: "http://localhost:11434"
  voice_ref: "assets/voice-reference.wav"
  voice_ref_text: "This is a sample of my voice for cloning purposes."
  music_file: "assets/background-music.wav"

segments:
  - id: introduction
    segment_type: music_only
    description: "Title Screen"
    steps:
      - img_introduction
  - id: hook
    segment_type: narration_only
    description: "Opening Hook"
    steps:
      - script_hook
  - id: problem
    segment_type: narration_only
    description: "Problem Statement"
    steps:
      - script_problem
  - id: solution
    segment_type: narration_only
    description: "Solution Overview"
    steps:
      - script_solution
  - id: architecture
    segment_type: narration_only
    description: "Core Concepts"
    steps:
      - script_architecture
  - id: step_types
    segment_type: narration_only
    description: "Step Types Overview"
    steps:
      - script_step_types
  - id: service_architecture
    segment_type: narration_only
    description: "Service Architecture"
    steps:
      - script_service_architecture
  - id: setup_repository
    segment_type: narration_only
    description: "Repository Structure"
    steps:
      - script_setup_repository
  - id: setup_new_project
    segment_type: narration_only
    description: "Creating a New Project"
    steps:
      - script_setup_new_project
  - id: workflow_yaml
    segment_type: narration_only
    description: "Workflow YAML Structure"
    steps:
      - script_workflow_yaml
  - id: voice_reference
    segment_type: narration_only
    description: "Voice Reference Setup"
    steps:
      - script_voice_reference
  - id: basic_execution
    segment_type: narration_only
    description: "Basic Execution"
    steps:
      - script_basic_execution
  - id: command_safety
    segment_type: narration_only
    description: "Command Safety"
    steps:
      - script_command_safety
  - id: resume_mode
    segment_type: narration_only
    description: "Resume Mode"
    steps:
      - script_resume_mode
  - id: variable_overrides
    segment_type: narration_only
    description: "Variable Overrides"
    steps:
      - script_variable_overrides
  - id: tts_voxcpm
    segment_type: narration_only
    description: "Text-to-Speech with VoxCPM"
    steps:
      - script_tts_voxcpm
  - id: image_generation_flux1
    segment_type: narration_only
    description: "Image Generation with FLUX.1"
    steps:
      - script_image_generation_flux1
  - id: video_generation
    segment_type: narration_only
    description: "Video Generation"
    steps:
      - script_video_generation
  - id: local_llm_ollama
    segment_type: narration_only
    description: "Local LLM with Ollama"
    steps:
      - script_local_llm_ollama
  - id: audio_processing
    segment_type: narration_only
    description: "Audio Processing"
    steps:
      - script_audio_processing
  - id: video_assembly
    segment_type: narration_only
    description: "Video Assembly"
    steps:
      - script_video_assembly
  - id: qa_llm_audit
    segment_type: narration_only
    description: "Quality Assurance with llm_audit"
    steps:
      - script_qa_llm_audit
  - id: meta_demonstration
    segment_type: narration_only
    description: "Meta Demonstration"
    steps:
      - script_meta_demonstration
  - id: dogfooding_philosophy
    segment_type: narration_only
    description: "Dogfooding Philosophy"
    steps:
      - script_dogfooding_philosophy
  - id: getting_started
    segment_type: narration_only
    description: "Getting Started"
    steps:
      - script_getting_started
  - id: resources
    segment_type: narration_only
    description: "Resources"
    steps:
      - script_resources
  - id: call_to_action
    segment_type: narration_only
    description: "Call to Action"
    steps:
      - script_call_to_action
  - id: outro
    segment_type: music_only
    description: "Closing"
    steps:
      - img_outro

steps:
  # ========== Directory Setup ==========
  - id: setup_dirs
    kind: ensure_dirs
    dirs:
      - "work/scripts"
      - "work/audio"
      - "work/images"
      - "work/videos"
      - "work/clips"
      - "work/reports"
      - "output"

  # ========== Scripts ==========
  - id: script_hook
    kind: write_file
    path: "work/scripts/01-hook.txt"
    content: |
      What if you could describe a video in plain text, and an AI would build it for you? No timeline editors. No render queues. Just write what you want, run one command, and get a finished video. That's exactly what Vee Double-You Eff does. And the video you're watching right now was produced entirely by this framework.

  - id: script_problem
    kind: write_file
    path: "work/scripts/02-problem.txt"
    content: |
      So, let's talk about the problems in traditional video production. Imagine spending hours meticulously arranging clips on a timeline editor. It's time-consuming and requires manual attention to every detail. Now, think about managing assets—versions, naming conventions, organization—that can quickly become chaotic.
      
      Repetitive tasks are another headache. You might spend countless hours generating text-to-speech audio or running the same command over and over. And what if you want to incorporate AI tools? They exist, but they often require manual orchestration, adding more layers of complexity.
      
      Have you ever encountered "agent drift"? It happens when AI assistants lose context or make inconsistent decisions. One day your assistant might generate perfect text-to-speech audio, and the next day, it's off. Not reproducible, right? You can't easily recreate or modify a video once it's done.
      
      Traditional methods fall short in several areas—efficiency, consistency, and scalability. That's where Vee Double-You Eff comes in. It's designed to solve these problems by offering a declarative approach to video production, using YAML workflows that define every step clearly. This ensures reproducibility and eliminates the issue of "agent drift."

  - id: script_solution
    kind: write_file
    path: "work/scripts/03-solution.txt"
    content: |
      Let's dive into the solution that addresses these challenges: Vee Double-You Eff, a Rust-based workflow engine for AI-powered video automation.
      
      Imagine defining your entire video production process in plain yaml files. No more timeline editors; just write what you want, run one command, and get a finished video. This is how VWF works.
      
      VWF orchestrates multiple AI services seamlessly:
      - Text-to-speech with VoxCPM
      - Text-to-image generation with FLUX.1
      - Image-to-video animations with SVD-XT
      - Direct text-to-video rendering via Wan 2.2
      
      The key innovation here is that workflows are data, not code. This means "agent drift" is eliminated. AI assistants always follow the same instructions, making decisions consistently and reproducibly.
      
      For even more efficiency, VWF supports incremental builds. With the --resume flag, only changed parts of the workflow are re-run. This saves a lot of time, especially on large projects.
      
      And to ensure quality, VWF includes self-auditing capabilities. Vision models validate produced assets, ensuring everything looks and sounds as expected. If there's an issue, the process will halt, preventing defective videos from going live.
      
      Finally, Vee Double-You Eff is fully open source and extensible. You can contribute new step types or modify existing ones to suit your needs. The community is growing, and contributions are welcome.
      
      So, if you're a developer interested in AI-powered video automation, a content creator looking to streamline production, or simply someone who wants to explore the potential of workflow engines, Vee Double-You Eff is for you. Let's see how it works in practice next.

  - id: script_architecture
    kind: write_file
    path: "work/scripts/04-architecture.txt"
    content: |
      In today's video, we'll dive into the architecture of Vee Double-You Eff, exploring its core concepts and how they come together to create a powerful workflow engine.
      
      Vee Double-You Eff is built on a three-layer architecture: vwf-config, vwf-steps, and vwf-core. Let's start with vwf-config. This layer handles YAML parsing, defines step types, and validates the workflow structure. It ensures that every step in your video production process is clearly defined and follows best practices.
      
      Next, we have vwf-steps. This layer contains the actual implementations of each step. Each step is defined in its own file, making the codebase modular and easy to manage. You can extend Vee Double-You Eff by adding new steps or modifying existing ones.
      
      At the heart of Vee Double-You Eff lies vwf-core. This is where the magic happens—the runner executes steps in order with dependency tracking. All side effects are handled through traits, making it easier to test and maintain the codebase. Incremental builds are supported using the --resume flag, which saves time by skipping completed steps.
      
      Finally, we have the command line interface (CLI) and the future web user interface (vwf-web). The CLI allows you to run workflows from the terminal, while vwf-web will provide a more interactive experience once it's developed.
      
      Together, these layers form a robust framework that orchestrates multiple AI services like text-to-speech, text-to-image, image-to-video, and text-to-video. With Vee Double-You Eff, you can describe your video in plain text and let the AI do the heavy lifting, streamlining your workflow and saving time on repetitive tasks.

  - id: script_step_types
    kind: write_file
    path: "work/scripts/05-step_types.txt"
    content: |
      Welcome to the Step Types Overview segment of our Vee Double-You Eff video explainer.
      
      In this section, we'll take a deep dive into the various step types available in VWF. These steps are the building blocks that allow you to define and execute your video workflows declaratively using yaml files.
      
      First up is ensure_dirs. This step creates the necessary directory structure for your project. Simply specify the paths you want to create, and VWF will handle the rest.
      
      Next is write_file. Use this to write text content to files. Ideal for generating scripts or other plain text assets that are part of your video production process.
      
      Split_sections comes in handy when you have a large text file and need to break it into manageable segments. This step allows you to specify delimiters, making it easy to split the content accordingly.
      
      For executing shell commands, we have run_command. Just provide the command you want to run, along with any necessary arguments. The --allow flag ensures that only specific commands are permitted, enhancing security.
      
      Llm_generate is used for generating text via Ollama, a local large language model. Perfect for automating scriptwriting or creating dynamic content within your workflows.
      
      Tts_generate handles voice cloning through VoxCPM. By providing reference audio, you can generate high-quality TTS narration that matches the style and tone of your desired voice.
      
      Text_to_image leverages FLUX.1 to generate images from text prompts. Whether you need illustrations or concept art, this step is invaluable for visual storytelling within your videos.
      
      Image_to_video uses SVD-XT to animate still images. This step is great for creating motion graphics or transitions that enhance the dynamic elements of your video content.
      
      Text_to_video generates videos directly from text prompts using Wan 2.2. Ideal for introducing scenes, narrating stories, or providing visual explanations without the need for traditional animation tools.
      
      Normalize_volume ensures that audio levels are consistent across your video. By targeting a specific dB level, you can maintain professional sound quality throughout your production.
      
      Audio_mix allows you to overlay background music with narration. By controlling the volume levels of each track, you can create balanced and engaging audio experiences.
      
      Video_concat joins multiple video clips into a single output file. This step is essential for assembling final videos from various segments or scenes.
      
      Create_slide generates title/text slides automatically. Perfect for creating visually appealing introductions or summaries within your video content.
      
      Whisper_transcribe transcribes audio to text using the Whisper model. Useful for generating closed captions, transcripts, or even for automated transcription during video creation.
      
      Lastly, llm_audit validates assets with vision models. By analyzing images and videos, this step helps ensure that all generated content meets quality standards, catching any issues early in the workflow.
      
      These step types, along with their respective parameters, provide a powerful toolkit for automating and streamlining your video production process. With Vee Double-You Eff, you can leverage AI services efficiently while maintaining full control over your creative vision.

  - id: script_service_architecture
    kind: write_file
    path: "work/scripts/06-service_architecture.txt"
    content: |
      Now, let's dive into how Vee Double-You Eff orchestrates its services. Imagine a network of interconnected tools, each specialized for a particular task in video production.
      
      At the heart of this system is Ollama, running locally on port 11434. It handles large language models for text generation and vision tasks. Next, we have VoxCPM on curiosity:7860, responsible for voice cloning through text-to-speech.
      
      FLUX.1, hosted on gpu-server:8570, is our go-to for text-to-image conversions, creating stunning visuals from simple prompts. SVD-XT, located on gpu-server:8100, takes still images and animates them into dynamic video segments using ComfyUI.
      
      Lastly, Wan 2.2, accessible at gpu-server:6000, generates videos directly from text prompts, offering a powerful way to bring scripts to life visually.
      
      All these services are containerized with Docker, ensuring they run smoothly and reliably across different environments. This distributed setup allows for efficient resource utilization and easy scalability as your video projects grow in complexity.
      
      Together, these components form the backbone of Vee Double-You Eff, enabling seamless integration and execution of various AI tasks, from text generation to image and video creation.

  - id: script_setup_repository
    kind: write_file
    path: "work/scripts/07-setup_repository.txt"
    content: |
      Now let's take a look at the repository structure of Vee Double-You Eff. The codebase is organized into several key directories. In the components/vwf-engine folder, you'll find the core library crates that make up the framework. Over in components/vwf-apps, you'll see the CLI and web applications. The docs directory contains all the documentation and LLM context needed to get started. Projects are stored in the projects/ directory, where each video project has its own folder. And finally, examples/ holds a variety of sample workflows to help you understand how things work. This organized structure makes it easy to navigate and contribute to the Vee Double-You Eff codebase.

  - id: script_setup_new_project
    kind: write_file
    path: "work/scripts/08-setup_new_project.txt"
    content: |
      Alright, let's dive into creating a new project with Vee Double-You Eff.
      
      First, you'll want to create a new directory for your project. Just open your terminal and run:
      
      ```bash
      mkdir my_video_project
      cd my_video_project
      ```
      
      Next, you need to write a brief description of the video you want to create. This goes into a file called `brief.txt`. Here's an example:
      
      ```
      This video will demonstrate the capabilities of Vee Double-You Eff.
      It will show how to set up a project and run workflows.
      ```
      
      Now that you have your brief, it's time to generate the initial workflow.yaml file. Use the vwf CLI command:
      
      ```bash
      vwf generate --brief brief.txt --output workflow.yaml
      ```
      
      This command reads your `brief.txt` and creates a `workflow.yaml` file with all the necessary steps predefined. Open this file in your text editor. You'll see that it includes sections for version, name, variables, segments, and steps.
      
      Review and customize the generated workflow to fit your needs. For example, you might want to add more details to the steps or adjust the parameters.
      
      One important step is setting up voice cloning for text-to-speech. Record 20-60 seconds of clean speech and provide an accurate transcript as `reference_text`. Place this audio file in the `assets/` directory. Then reference it in your workflow.yaml like this:
      
      ```yaml
      voice_reference: "{{ assets/voice-reference.wav }}"
      ```
      
      That's it! You've set up a new project with Vee Double-You Eff. The next steps will involve running the workflow and customizing it further based on your requirements.

  - id: script_workflow_yaml
    kind: write_file
    path: "work/scripts/09-workflow_yaml.txt"
    content: |
      Alright, let's dive into the heart of Vee Double-You Eff: the workflow.yaml file. This is where everything comes together.
      
      Imagine you're writing a script for a movie. Instead of using a timeline editor, you describe each scene in plain text. That's exactly what we do with YAML. It's like a blueprint for your video production.
      
      Open up your workflow.yaml and let’s break it down.
      
      At the top, you have the version and name fields. These are just identifiers, so Vee Double-You Eff knows which version of the file it's dealing with and what to call this particular workflow.
      
      Next is the vars section. Think of this like defining variables in a programming language. You can reuse these values throughout your workflow. For example, you might define project_name or output_dir here.
      
      Now, we move on to segments. These are used for organizing audio content. You have music_only, narration_only, and mixed segments. This helps keep things tidy and ensures that each part of your video is handled correctly.
      
      The main component of the workflow.yaml file is the steps array. Each step has an id, kind, and parameters. The id is just a unique identifier for that step. Kind tells Vee Double-You Eff what type of action to perform—like running a command, generating text, or creating images. And parameters are the specific inputs needed for that action.
      
      For example, if you have a step with kind: tts_generate, you might specify the text to be spoken and the output file name.
      
      The resume_output field is crucial for incremental builds. If Vee Double-You Eff finds this output already exists, it skips that step. This saves time, especially on large workflows or when you need to make small changes.
      
      And finally, you have template variables. These are enclosed in double curly braces and get replaced with actual values at runtime. For instance, {{project_name}} gets replaced with the value defined in your vars section.
      
      So, there you have it—the workflow.yaml file. It's where you declare every step of your video production process. By defining everything in YAML, Vee Double-You Eff ensures that your workflows are reproducible and easy to audit. No more agent drift—just clear data-driven steps.
      
      Next up, we'll talk about setting up the voice reference for text-to-speech. This is essential for making sure your AI-generated audio sounds just right.

  - id: script_voice_reference
    kind: write_file
    path: "work/scripts/10-voice_reference.txt"
    content: |
      Alright, let's talk about voice cloning with VoxCPM for our TTS process.
      
      First up, you'll need to record 20 to 60 seconds of clean speech. Make sure it's clear and natural, without any background noise or interruptions. This will serve as your reference audio.
      
      Next, provide an accurate transcript of the recording. This text should match what was spoken in the audio file word for word. It’s crucial for getting the voice cloning just right.
      
      Now, place this reference audio file in your project's assets/ directory. Make sure it’s named something descriptive, like "voice-reference.wav."
      
      Finally, you’ll want to reference this voice audio in your workflow.yaml file using a template variable. For example, you might use {{ voice_reference }} where you need the path to the audio.
      
      That’s it! With these steps, VoxCPM will clone your voice for text-to-speech, ensuring that the narration sounds just like you.

  - id: script_basic_execution
    kind: write_file
    path: "work/scripts/11-basic_execution.txt"
    content: |
      Running a workflow with Vee Double-You Eff is straightforward. First, you preview your workflow using the --dry-run flag. This lets you see what steps will be executed without actually running them. The command looks like this: `vwf run workflow.yaml --workdir . --dry-run`.
      
      When you're ready to execute for real, simply remove the --dry-run flag and run: `vwf run workflow.yaml --workdir .`. You'll see output in your terminal for each step as it's processed. The framework handles dependencies automatically, so steps are executed in the correct order.
      
      The timing of each step is displayed, giving you a clear picture of how long each part of your video production takes. After execution, a run.json manifest is generated. This file contains a detailed audit trail of all operations performed during the workflow run, which is invaluable for debugging and verification.

  - id: script_command_safety
    kind: write_file
    path: "work/scripts/12-command_safety.txt"
    content: |
      In today's segment, let's talk about command safety in Vee Double-You Eff.
      
      When you run a workflow, some steps might require executing shell commands. To ensure security, these commands need explicit permission. This is where the --allow flag comes into play.
      
      For example, if your workflow includes an ffmpeg step for audio normalization, you'd use the --allow flag like this:
      
      ```
      vwf run workflow.yaml --workdir . --allow ffmpeg
      ```
      
      This way, only explicitly allowed programs can be executed. It's a powerful feature that helps prevent accidental execution of unauthorized commands.
      
      If you try to run a command without the necessary permission, Vee Double-You Eff will throw an error and guide you on how to add the required flag. This ensures that your workflows are secure and that you have full control over what gets executed.
      
      By using the --allow flag system, you can trust your workflows while maintaining tight security controls. It's a key aspect of Vee Double-You Eff's robust design, making it safe and reliable for even complex video production tasks.

  - id: script_resume_mode
    kind: write_file
    path: "work/scripts/13-resume_mode.txt"
    content: |
      Incremental builds with the --resume flag save you time on re-runs. When you execute a workflow, each step is checked against its resume_output. If the output already exists, that step is skipped. This means only changed or failed steps are re-run after interruptions or crashes. It's especially useful for large workflows, significantly speeding up the process. Just use vwf run with the --resume option to take advantage of this feature and streamline your workflow execution.

  - id: script_variable_overrides
    kind: write_file
    path: "work/scripts/14-variable_overrides.txt"
    content: |
      Runtime customization with variable overrides:
      
      Use the --var flag to change workflow values on the fly.
      For example: vwf run workflow.yaml --var project_name=Demo
      
      This allows you to batch process multiple projects easily.
      
      Template variables like {{project_name}} render at execution time.
      
      Great for dynamic workflows and reducing configuration duplication.

  - id: script_tts_voxcpm
    kind: write_file
    path: "work/scripts/15-tts_voxcpm.txt"
    content: |
      Voice cloning explained:
      VoxCPM uses reference audio for voice matching.
      Narration style guidelines: acronyms, pacing.
      Common issues: dropouts, pronunciation errors.
      Using whisper_transcribe to verify output.

  - id: script_image_generation_flux1
    kind: write_file
    path: "work/scripts/16-image_generation_flux1.txt"
    content: |
      Welcome to our segment on image generation with FLUX.1.
      
      FLUX.1 is a powerful tool for turning plain text into stunning images. It’s like having a digital artist at your fingertips. Imagine describing an image in words, and FLUX.1 brings it to life—fast and efficiently.
      
      Here's how it works: you start by crafting a detailed prompt. This can be anything from a description of a landscape to a complex scene with multiple elements. The more specific and vivid your text, the better the result.
      
      FLUX.1 uses advanced AI algorithms to interpret your prompt and generate an image. It’s incredibly fast, making it perfect for iterative design processes. You can quickly test different ideas by tweaking your prompt and regenerating the image.
      
      One of the key features is reproducibility. By setting a seed value in your prompt, you can ensure that FLUX.1 generates the same image every time. This consistency is invaluable when iterating on designs or creating variations.
      
      Orientation options are another handy feature. You can specify whether you want a portrait, landscape, or square image. This flexibility allows you to tailor each generated image to fit your specific needs.
      
      But it’s not just about speed and reproducibility; FLUX.1 also offers high-quality results. The images produced are detailed and realistic, often indistinguishable from those created by human artists.
      
      To get started with FLUX.1, you’ll need to define a step in your workflow.yaml file. Here, you specify the kind as "text_to_image" and provide the prompt along with any additional parameters like orientation and seed value.
      
      Remember, the quality of the output depends heavily on the clarity and specificity of your prompt. So spend some time refining your description before hitting generate. With FLUX.1, the possibilities are truly endless—your imagination is the only limit.

  - id: script_video_generation
    kind: write_file
    path: "work/scripts/17-video_generation.txt"
    content: |
      Now let's dive into video generation with Wan 2.2. Imagine creating a video just by describing it in plain text. That's exactly what we're doing here.
      
      Wan 2.2 is our go-to tool for text-to-video conversion. It takes your textual description and turns it into a visual masterpiece. This is particularly useful when you want to convey complex ideas or tell a story without the hassle of traditional video editing.
      
      To use Wan 2.2, you simply define a step in your workflow.yaml file with the kind set to "text_to_video". You provide the prompt as a parameter, which could be anything from a simple sentence to a detailed script. For instance:
      
      ```yaml
      - id: generate_intro
        kind: text_to_video
        params:
          prompt: "Show an animated title card with 'Vee Double-You Eff' in neon lights."
      ```
      
      When you run your workflow, Wan 2.2 processes this prompt and generates a video segment accordingly. It's like magic – just describe what you want, and the AI does the rest.
      
      The beauty of using text-to-video is its flexibility. You can control various parameters such as frame count, motion intensity, and CFG scale to fine-tune the output. For example:
      
      ```yaml
      - id: generate_scene
        kind: text_to_video
        params:
          prompt: "Create a scene of a dog eating food in a park."
          frames: 120
          motion_intensity: 50
          cfg_scale: 7.5
      ```
      
      This approach allows you to visualize your ideas quickly and efficiently, making video production more accessible than ever before.
      
      Remember, the key is clear communication. The better your prompt is crafted, the more accurate and engaging the generated video will be. So spend some time refining your descriptions to get the best results.
      
      In summary, Wan 2.2 is a powerful tool for turning text into stunning videos. It's just one of many features in Vee Double-You Eff that streamlines the video creation process, making it faster and more enjoyable for content creators like you.

  - id: script_local_llm_ollama
    kind: write_file
    path: "work/scripts/18-local_llm_ollama.txt"
    content: |
      Using Ollama for local large language model processing is a game-changer. With llm_generate, you can script writing and prompt engineering right in your workflow. For vision-based asset validation, llm_audit extracts frames from videos and sends them to vision models like llava or qwen2.5vl. All processing stays local, ensuring privacy and security. Available models include qwen2.5-coder, llava, and llama3.2-vision. This integration makes Vee Double-You Eff incredibly powerful and flexible for a wide range of video production needs.

  - id: script_audio_processing
    kind: write_file
    path: "work/scripts/19-audio_processing.txt"
    content: |
      Alright, let's dive into audio processing.
      
      Audio quality is crucial in any video production. Vee Double-You Eff handles this with precision using a few key steps.
      
      First up, `normalize_volume` ensures that your narration sits at an optimal level, typically around negative twenty-five decibels for clarity and impact.
      
      Next, `audio_mix` allows you to seamlessly overlay background music with your narration. Music is mixed at a lower level, usually negative thirty-two decibels, to maintain balance without overpowering the spoken words.
      
      Segment organization in your workflow is essential. By clearly delineating segments as `music_only`, `narration_only`, and `mixed`, you prevent unwanted overlaps and ensure smooth transitions between elements.
      
      For a polished finish, use `fade_out` on music tracks to create natural endings that don't abruptly cut off, enhancing the overall audio experience.
      
      These steps are all managed through your workflow.yaml file, ensuring consistent and high-quality audio production every time.

  - id: script_video_assembly
    kind: write_file
    path: "work/scripts/20-video_assembly.txt"
    content: |
      In final video production, we bring everything together. The `create_slide` step generates titles and text cards. Then, `video_concat` joins all segments into one cohesive piece. For mixed codecs, the `reencode` option ensures compatibility. Finally, export your video to the desired delivery format. This streamlined process guarantees a polished, professional final product ready for sharing.

  - id: script_qa_llm_audit
    kind: write_file
    path: "work/scripts/21-qa_llm_audit.txt"
    content: |
      In this segment, we dive into one of Vee Double-You Eff's most powerful features: quality assurance with llm_audit. This step ensures that every asset produced by the framework is top-notch.
      
      Imagine you've spent hours generating a video, only to find out later that some frames are corrupted or artifacts are present. With llm_audit, this becomes a thing of the past. It works by extracting frames from videos at regular intervals and sending them to vision models like llava and qwen2.5vl for analysis.
      
      These models check for issues such as corruption, artifacts, and even blank frames. If any problems are detected, llm_audit can halt the workflow entirely with the fail_on_issues flag. This prevents further processing of faulty assets, saving you time and resources.
      
      But that's not all. The llm_audit step also generates a comprehensive JSON report detailing the findings. You can review this report to make informed decisions about how to proceed with your video production.
      
      In essence, llm_audit acts as a final gatekeeper for your media assets, ensuring they meet the highest standards before they reach your audience. This feature is particularly valuable when working with complex workflows that involve multiple AI services.
      
      So, the next time you run your Vee Double-You Eff workflow, remember to include the llm_audit step. It's a small investment in quality assurance that can save you from costly mistakes down the line.

  - id: script_meta_demonstration
    kind: write_file
    path: "work/scripts/22-meta_demonstration.txt"
    content: |
      Now, let's dive into how this video was actually created using Vee Double-You Eff itself. Imagine you're looking at a YAML file on your screen. That's exactly what we have here.
      
      First, we generated the script for this video using the `llm_generate` step from Ollama. This LLM took our initial brief and expanded it into detailed narration text. It’s like having an AI writing assistant who knows exactly how to craft engaging content.
      
      Next came the text-to-speech generation with VoxCPM. We provided a voice reference audio file, and VoxCPM cloned that voice for the narration. The result is clear and consistent throughout the entire video, just as we would expect from a human narrator.
      
      For the title animation, we used Wan 2.2, which generated a visually appealing sequence based on our text prompts. It’s like having a designer who can create professional graphics in seconds.
      
      The background music was mixed using the `audio_mix` step. We overlaid an ambient track at -32 dB and ensured proper fades to keep everything smooth and cohesive.
      
      Finally, all the segments were assembled using the `video_concat` step. We created slides for titles and text cards with `create_slide`, and then combined everything into a final video.
      
      The entire workflow was defined in a single YAML file, which we can share with others so they can reproduce or modify it as needed. This is the power of Vee Double-You Eff: turning plain text into polished videos with minimal effort.
      
      So, next time you’re working on a video project, remember that you could be just as productive by using VWF to automate your workflow. It’s not just about saving time; it's also about ensuring consistency and reproducibility in every step of the process.

  - id: script_dogfooding_philosophy
    kind: write_file
    path: "work/scripts/23-dogfooding_philosophy.txt"
    content: |
      Eating our own dogfood means using Vee Double-You Eff to create this video. It finds gaps in functionality, ensures real-world usability, and documents by example. Plus, it builds confidence in the tool. We used llm_generate for script writing, VoxCPM for TTS, Wan 2.2 for title animation, and more. This hands-on approach shows you exactly how Vee Double-You Eff works in practice.

  - id: script_getting_started
    kind: write_file
    path: "work/scripts/24-getting_started.txt"
    content: |
      Alright, let's get started! 
      
      To begin using Vee Double-You Eff, first clone the repository from GitHub at github dot com slash softwarewrighter slash video workflow underscore R-S.
      
      Next, you'll need to install Rust and cargo on your machine if you haven't already. This will allow you to compile and run the VWF framework locally.
      
      Once Rust is set up, navigate to the cloned directory in your terminal and run `cargo build` to compile the project. This step might take a few minutes depending on your system's performance.
      
      After building, verify everything is working by running some example workflows provided in the examples folder. These will help you understand how different parts of VWF come together.
      
      For your first workflow, start with something simple. Create a new directory for your project and write a brief description of the video you want to produce in a text file called `brief.txt`.
      
      Then, use the `vwf generate` command to create an initial `workflow.yaml` file. This YAML configuration will define every step of your video production process.
      
      Review and customize this workflow as needed. You might need to add voice reference audio for the text-to-speech generation or adjust parameters for image and video generation services.
      
      Once you're ready, run your workflow using `vwf run workflow.yaml --workdir .`. The dry-run mode with `--dry-run` can be useful to preview what steps will be executed before actually running them.
      
      That's it! You've successfully set up and started a project with Vee Double-You Eff. As you get more comfortable, explore the various step types and services available to create complex and automated video productions.

  - id: script_resources
    kind: write_file
    path: "work/scripts/25-resources.txt"
    content: |
      So where can you learn more about Vee Double-You Eff? Start with the tutorial in docs/tutorial.md for step-by-step guidance. For service setup details, check out docs/services.md. Explore example workflows in examples/workflows/ to see how others have used the framework. If you run into issues or want new features, head over to GitHub and open an issue. Whether you're a developer, content creator, or Rust programmer, Vee Double-You Eff has something for everyone. Thanks for watching!

  - id: script_call_to_action
    kind: write_file
    path: "work/scripts/26-call_to_action.txt"
    content: |
      Try Vee Double-You Eff today. Clone it from GitHub at github dot com slash softwarewrighter slash video workflow underscore R-S. Star it if you find it useful, and open an issue if you have questions or feature requests. Thanks for watching!

  # ========== TTS Generation ==========
  - id: tts_hook
    kind: tts_generate
    resume_output: "work/audio/01-hook.wav"
    script_path: "work/scripts/01-hook.txt"
    output_path: "work/audio/01-hook.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_problem
    kind: tts_generate
    resume_output: "work/audio/02-problem.wav"
    script_path: "work/scripts/02-problem.txt"
    output_path: "work/audio/02-problem.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_solution
    kind: tts_generate
    resume_output: "work/audio/03-solution.wav"
    script_path: "work/scripts/03-solution.txt"
    output_path: "work/audio/03-solution.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_architecture
    kind: tts_generate
    resume_output: "work/audio/04-architecture.wav"
    script_path: "work/scripts/04-architecture.txt"
    output_path: "work/audio/04-architecture.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_step_types
    kind: tts_generate
    resume_output: "work/audio/05-step_types.wav"
    script_path: "work/scripts/05-step_types.txt"
    output_path: "work/audio/05-step_types.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_service_architecture
    kind: tts_generate
    resume_output: "work/audio/06-service_architecture.wav"
    script_path: "work/scripts/06-service_architecture.txt"
    output_path: "work/audio/06-service_architecture.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_setup_repository
    kind: tts_generate
    resume_output: "work/audio/07-setup_repository.wav"
    script_path: "work/scripts/07-setup_repository.txt"
    output_path: "work/audio/07-setup_repository.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_setup_new_project
    kind: tts_generate
    resume_output: "work/audio/08-setup_new_project.wav"
    script_path: "work/scripts/08-setup_new_project.txt"
    output_path: "work/audio/08-setup_new_project.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_workflow_yaml
    kind: tts_generate
    resume_output: "work/audio/09-workflow_yaml.wav"
    script_path: "work/scripts/09-workflow_yaml.txt"
    output_path: "work/audio/09-workflow_yaml.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_voice_reference
    kind: tts_generate
    resume_output: "work/audio/10-voice_reference.wav"
    script_path: "work/scripts/10-voice_reference.txt"
    output_path: "work/audio/10-voice_reference.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_basic_execution
    kind: tts_generate
    resume_output: "work/audio/11-basic_execution.wav"
    script_path: "work/scripts/11-basic_execution.txt"
    output_path: "work/audio/11-basic_execution.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_command_safety
    kind: tts_generate
    resume_output: "work/audio/12-command_safety.wav"
    script_path: "work/scripts/12-command_safety.txt"
    output_path: "work/audio/12-command_safety.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_resume_mode
    kind: tts_generate
    resume_output: "work/audio/13-resume_mode.wav"
    script_path: "work/scripts/13-resume_mode.txt"
    output_path: "work/audio/13-resume_mode.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_variable_overrides
    kind: tts_generate
    resume_output: "work/audio/14-variable_overrides.wav"
    script_path: "work/scripts/14-variable_overrides.txt"
    output_path: "work/audio/14-variable_overrides.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_tts_voxcpm
    kind: tts_generate
    resume_output: "work/audio/15-tts_voxcpm.wav"
    script_path: "work/scripts/15-tts_voxcpm.txt"
    output_path: "work/audio/15-tts_voxcpm.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_image_generation_flux1
    kind: tts_generate
    resume_output: "work/audio/16-image_generation_flux1.wav"
    script_path: "work/scripts/16-image_generation_flux1.txt"
    output_path: "work/audio/16-image_generation_flux1.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_video_generation
    kind: tts_generate
    resume_output: "work/audio/17-video_generation.wav"
    script_path: "work/scripts/17-video_generation.txt"
    output_path: "work/audio/17-video_generation.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_local_llm_ollama
    kind: tts_generate
    resume_output: "work/audio/18-local_llm_ollama.wav"
    script_path: "work/scripts/18-local_llm_ollama.txt"
    output_path: "work/audio/18-local_llm_ollama.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_audio_processing
    kind: tts_generate
    resume_output: "work/audio/19-audio_processing.wav"
    script_path: "work/scripts/19-audio_processing.txt"
    output_path: "work/audio/19-audio_processing.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_video_assembly
    kind: tts_generate
    resume_output: "work/audio/20-video_assembly.wav"
    script_path: "work/scripts/20-video_assembly.txt"
    output_path: "work/audio/20-video_assembly.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_qa_llm_audit
    kind: tts_generate
    resume_output: "work/audio/21-qa_llm_audit.wav"
    script_path: "work/scripts/21-qa_llm_audit.txt"
    output_path: "work/audio/21-qa_llm_audit.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_meta_demonstration
    kind: tts_generate
    resume_output: "work/audio/22-meta_demonstration.wav"
    script_path: "work/scripts/22-meta_demonstration.txt"
    output_path: "work/audio/22-meta_demonstration.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_dogfooding_philosophy
    kind: tts_generate
    resume_output: "work/audio/23-dogfooding_philosophy.wav"
    script_path: "work/scripts/23-dogfooding_philosophy.txt"
    output_path: "work/audio/23-dogfooding_philosophy.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_getting_started
    kind: tts_generate
    resume_output: "work/audio/24-getting_started.wav"
    script_path: "work/scripts/24-getting_started.txt"
    output_path: "work/audio/24-getting_started.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_resources
    kind: tts_generate
    resume_output: "work/audio/25-resources.wav"
    script_path: "work/scripts/25-resources.txt"
    output_path: "work/audio/25-resources.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  - id: tts_call_to_action
    kind: tts_generate
    resume_output: "work/audio/26-call_to_action.wav"
    script_path: "work/scripts/26-call_to_action.txt"
    output_path: "work/audio/26-call_to_action.wav"
    server: "{{tts_server}}"
    reference_audio: "{{voice_ref}}"
    reference_text: "{{voice_ref_text}}"

  # ========== Visual Generation ==========
  - id: img_introduction
    kind: create_slide
    resume_output: "work/images/00-introduction.png"
    template: title
    text: "Title Screen"
    output_path: "work/images/00-introduction.png"
    orientation: landscape

  - id: img_hook
    kind: create_slide
    resume_output: "work/images/01-hook.png"
    template: title
    text: "Opening Hook"
    background_color: "#1a1a2e"
    output_path: "work/images/01-hook.png"
    orientation: landscape

  - id: img_problem
    kind: create_slide
    resume_output: "work/images/02-problem.png"
    template: title
    text: "Problem Statement"
    background_color: "#1a1a2e"
    output_path: "work/images/02-problem.png"
    orientation: landscape

  - id: img_solution
    kind: create_slide
    resume_output: "work/images/03-solution.png"
    template: title
    text: "Solution Overview"
    background_color: "#1a1a2e"
    output_path: "work/images/03-solution.png"
    orientation: landscape

  - id: img_architecture
    kind: text_to_image
    resume_output: "work/images/04-architecture.png"
    prompt: "Professional diagram showing core concepts, dark theme, clean modern design, high contrast"
    output_path: "work/images/04-architecture.png"
    orientation: landscape
    seed: 2030
    server: "{{tti_server}}"

  - id: img_step_types
    kind: text_to_image
    resume_output: "work/images/05-step_types.png"
    prompt: "Professional diagram showing step types overview, dark theme, clean modern design, high contrast"
    output_path: "work/images/05-step_types.png"
    orientation: landscape
    seed: 2031
    server: "{{tti_server}}"

  - id: img_service_architecture
    kind: create_slide
    resume_output: "work/images/06-service_architecture.png"
    template: title
    text: "Service Architecture"
    background_color: "#1a1a2e"
    output_path: "work/images/06-service_architecture.png"
    orientation: landscape

  - id: img_setup_repository
    kind: create_slide
    resume_output: "work/images/07-setup_repository.png"
    template: title
    text: "Repository Structure"
    background_color: "#1a1a2e"
    output_path: "work/images/07-setup_repository.png"
    orientation: landscape

  - id: img_setup_new_project
    kind: create_slide
    resume_output: "work/images/08-setup_new_project.png"
    template: title
    text: "Creating a New Project"
    background_color: "#1a1a2e"
    output_path: "work/images/08-setup_new_project.png"
    orientation: landscape

  - id: img_workflow_yaml
    kind: create_slide
    resume_output: "work/images/09-workflow_yaml.png"
    template: title
    text: "Workflow YAML Structure"
    background_color: "#1a1a2e"
    output_path: "work/images/09-workflow_yaml.png"
    orientation: landscape

  - id: img_voice_reference
    kind: create_slide
    resume_output: "work/images/10-voice_reference.png"
    template: title
    text: "Voice Reference Setup"
    background_color: "#1a1a2e"
    output_path: "work/images/10-voice_reference.png"
    orientation: landscape

  - id: img_basic_execution
    kind: create_slide
    resume_output: "work/images/11-basic_execution.png"
    template: title
    text: "Basic Execution"
    background_color: "#1a1a2e"
    output_path: "work/images/11-basic_execution.png"
    orientation: landscape

  - id: img_command_safety
    kind: create_slide
    resume_output: "work/images/12-command_safety.png"
    template: title
    text: "Command Safety"
    background_color: "#1a1a2e"
    output_path: "work/images/12-command_safety.png"
    orientation: landscape

  - id: img_resume_mode
    kind: create_slide
    resume_output: "work/images/13-resume_mode.png"
    template: title
    text: "Resume Mode"
    background_color: "#1a1a2e"
    output_path: "work/images/13-resume_mode.png"
    orientation: landscape

  - id: img_variable_overrides
    kind: create_slide
    resume_output: "work/images/14-variable_overrides.png"
    template: title
    text: "Variable Overrides"
    background_color: "#1a1a2e"
    output_path: "work/images/14-variable_overrides.png"
    orientation: landscape

  - id: img_tts_voxcpm
    kind: create_slide
    resume_output: "work/images/15-tts_voxcpm.png"
    template: title
    text: "Text-to-Speech with VoxCPM"
    background_color: "#1a1a2e"
    output_path: "work/images/15-tts_voxcpm.png"
    orientation: landscape

  - id: img_image_generation_flux1
    kind: create_slide
    resume_output: "work/images/16-image_generation_flux1.png"
    template: title
    text: "Image Generation with FLUX.1"
    background_color: "#1a1a2e"
    output_path: "work/images/16-image_generation_flux1.png"
    orientation: landscape

  - id: img_video_generation
    kind: create_slide
    resume_output: "work/images/17-video_generation.png"
    template: title
    text: "Video Generation"
    background_color: "#1a1a2e"
    output_path: "work/images/17-video_generation.png"
    orientation: landscape

  - id: img_local_llm_ollama
    kind: create_slide
    resume_output: "work/images/18-local_llm_ollama.png"
    template: title
    text: "Local LLM with Ollama"
    background_color: "#1a1a2e"
    output_path: "work/images/18-local_llm_ollama.png"
    orientation: landscape

  - id: img_audio_processing
    kind: create_slide
    resume_output: "work/images/19-audio_processing.png"
    template: title
    text: "Audio Processing"
    background_color: "#1a1a2e"
    output_path: "work/images/19-audio_processing.png"
    orientation: landscape

  - id: img_video_assembly
    kind: create_slide
    resume_output: "work/images/20-video_assembly.png"
    template: title
    text: "Video Assembly"
    background_color: "#1a1a2e"
    output_path: "work/images/20-video_assembly.png"
    orientation: landscape

  - id: img_qa_llm_audit
    kind: create_slide
    resume_output: "work/images/21-qa_llm_audit.png"
    template: title
    text: "Quality Assurance with llm_audit"
    background_color: "#1a1a2e"
    output_path: "work/images/21-qa_llm_audit.png"
    orientation: landscape

  - id: img_meta_demonstration
    kind: create_slide
    resume_output: "work/images/22-meta_demonstration.png"
    template: title
    text: "Meta Demonstration"
    background_color: "#1a1a2e"
    output_path: "work/images/22-meta_demonstration.png"
    orientation: landscape

  - id: img_dogfooding_philosophy
    kind: create_slide
    resume_output: "work/images/23-dogfooding_philosophy.png"
    template: title
    text: "Dogfooding Philosophy"
    background_color: "#1a1a2e"
    output_path: "work/images/23-dogfooding_philosophy.png"
    orientation: landscape

  - id: img_getting_started
    kind: create_slide
    resume_output: "work/images/24-getting_started.png"
    template: title
    text: "Getting Started"
    background_color: "#1a1a2e"
    output_path: "work/images/24-getting_started.png"
    orientation: landscape

  - id: img_resources
    kind: create_slide
    resume_output: "work/images/25-resources.png"
    template: title
    text: "Resources"
    background_color: "#1a1a2e"
    output_path: "work/images/25-resources.png"
    orientation: landscape

  - id: img_call_to_action
    kind: create_slide
    resume_output: "work/images/26-call_to_action.png"
    template: title
    text: "Call to Action"
    background_color: "#1a1a2e"
    output_path: "work/images/26-call_to_action.png"
    orientation: landscape

  - id: img_outro
    kind: create_slide
    resume_output: "work/images/27-outro.png"
    template: title
    text: "Closing"
    output_path: "work/images/27-outro.png"
    orientation: landscape

  # ========== Clip Assembly ==========
  - id: clip_introduction
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/00-introduction.png"
      - "-i"
      - "{{music_file}}"
      - "-filter_complex"
      - "[1:a]volume=-28dB,afade=t=out:st=4:d=1[aout]"
      - "-map"
      - "0:v"
      - "-map"
      - "[aout]"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-pix_fmt"
      - "yuv420p"
      - "-t"
      - "5"
      - "work/clips/00-introduction.mp4"

  - id: clip_hook
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/01-hook.png"
      - "-i"
      - "work/audio/01-hook.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/01-hook.mp4"

  - id: clip_problem
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/02-problem.png"
      - "-i"
      - "work/audio/02-problem.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/02-problem.mp4"

  - id: clip_solution
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/03-solution.png"
      - "-i"
      - "work/audio/03-solution.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/03-solution.mp4"

  - id: clip_architecture
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/04-architecture.png"
      - "-i"
      - "work/audio/04-architecture.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/04-architecture.mp4"

  - id: clip_step_types
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/05-step_types.png"
      - "-i"
      - "work/audio/05-step_types.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/05-step_types.mp4"

  - id: clip_service_architecture
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/06-service_architecture.png"
      - "-i"
      - "work/audio/06-service_architecture.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/06-service_architecture.mp4"

  - id: clip_setup_repository
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/07-setup_repository.png"
      - "-i"
      - "work/audio/07-setup_repository.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/07-setup_repository.mp4"

  - id: clip_setup_new_project
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/08-setup_new_project.png"
      - "-i"
      - "work/audio/08-setup_new_project.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/08-setup_new_project.mp4"

  - id: clip_workflow_yaml
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/09-workflow_yaml.png"
      - "-i"
      - "work/audio/09-workflow_yaml.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/09-workflow_yaml.mp4"

  - id: clip_voice_reference
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/10-voice_reference.png"
      - "-i"
      - "work/audio/10-voice_reference.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/10-voice_reference.mp4"

  - id: clip_basic_execution
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/11-basic_execution.png"
      - "-i"
      - "work/audio/11-basic_execution.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/11-basic_execution.mp4"

  - id: clip_command_safety
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/12-command_safety.png"
      - "-i"
      - "work/audio/12-command_safety.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/12-command_safety.mp4"

  - id: clip_resume_mode
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/13-resume_mode.png"
      - "-i"
      - "work/audio/13-resume_mode.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/13-resume_mode.mp4"

  - id: clip_variable_overrides
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/14-variable_overrides.png"
      - "-i"
      - "work/audio/14-variable_overrides.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/14-variable_overrides.mp4"

  - id: clip_tts_voxcpm
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/15-tts_voxcpm.png"
      - "-i"
      - "work/audio/15-tts_voxcpm.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/15-tts_voxcpm.mp4"

  - id: clip_image_generation_flux1
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/16-image_generation_flux1.png"
      - "-i"
      - "work/audio/16-image_generation_flux1.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/16-image_generation_flux1.mp4"

  - id: clip_video_generation
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/17-video_generation.png"
      - "-i"
      - "work/audio/17-video_generation.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/17-video_generation.mp4"

  - id: clip_local_llm_ollama
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/18-local_llm_ollama.png"
      - "-i"
      - "work/audio/18-local_llm_ollama.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/18-local_llm_ollama.mp4"

  - id: clip_audio_processing
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/19-audio_processing.png"
      - "-i"
      - "work/audio/19-audio_processing.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/19-audio_processing.mp4"

  - id: clip_video_assembly
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/20-video_assembly.png"
      - "-i"
      - "work/audio/20-video_assembly.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/20-video_assembly.mp4"

  - id: clip_qa_llm_audit
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/21-qa_llm_audit.png"
      - "-i"
      - "work/audio/21-qa_llm_audit.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/21-qa_llm_audit.mp4"

  - id: clip_meta_demonstration
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/22-meta_demonstration.png"
      - "-i"
      - "work/audio/22-meta_demonstration.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/22-meta_demonstration.mp4"

  - id: clip_dogfooding_philosophy
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/23-dogfooding_philosophy.png"
      - "-i"
      - "work/audio/23-dogfooding_philosophy.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/23-dogfooding_philosophy.mp4"

  - id: clip_getting_started
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/24-getting_started.png"
      - "-i"
      - "work/audio/24-getting_started.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/24-getting_started.mp4"

  - id: clip_resources
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/25-resources.png"
      - "-i"
      - "work/audio/25-resources.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/25-resources.mp4"

  - id: clip_call_to_action
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/26-call_to_action.png"
      - "-i"
      - "work/audio/26-call_to_action.wav"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-b:a"
      - "192k"
      - "-pix_fmt"
      - "yuv420p"
      - "-shortest"
      - "work/clips/26-call_to_action.mp4"

  - id: clip_outro
    kind: run_command
    program: ffmpeg
    args:
      - "-y"
      - "-loop"
      - "1"
      - "-i"
      - "work/images/27-outro.png"
      - "-i"
      - "{{music_file}}"
      - "-filter_complex"
      - "[1:a]volume=-28dB,afade=t=out:st=4:d=1[aout]"
      - "-map"
      - "0:v"
      - "-map"
      - "[aout]"
      - "-c:v"
      - "libx264"
      - "-tune"
      - "stillimage"
      - "-c:a"
      - "aac"
      - "-pix_fmt"
      - "yuv420p"
      - "-t"
      - "5"
      - "work/clips/27-outro.mp4"

  # ========== Final Assembly ==========
  - id: final_concat
    kind: video_concat
    clips:
      - "work/clips/00-introduction.mp4"
      - "work/clips/01-hook.mp4"
      - "work/clips/02-problem.mp4"
      - "work/clips/03-solution.mp4"
      - "work/clips/04-architecture.mp4"
      - "work/clips/05-step_types.mp4"
      - "work/clips/06-service_architecture.mp4"
      - "work/clips/07-setup_repository.mp4"
      - "work/clips/08-setup_new_project.mp4"
      - "work/clips/09-workflow_yaml.mp4"
      - "work/clips/10-voice_reference.mp4"
      - "work/clips/11-basic_execution.mp4"
      - "work/clips/12-command_safety.mp4"
      - "work/clips/13-resume_mode.mp4"
      - "work/clips/14-variable_overrides.mp4"
      - "work/clips/15-tts_voxcpm.mp4"
      - "work/clips/16-image_generation_flux1.mp4"
      - "work/clips/17-video_generation.mp4"
      - "work/clips/18-local_llm_ollama.mp4"
      - "work/clips/19-audio_processing.mp4"
      - "work/clips/20-video_assembly.mp4"
      - "work/clips/21-qa_llm_audit.mp4"
      - "work/clips/22-meta_demonstration.mp4"
      - "work/clips/23-dogfooding_philosophy.mp4"
      - "work/clips/24-getting_started.mp4"
      - "work/clips/25-resources.mp4"
      - "work/clips/26-call_to_action.mp4"
      - "work/clips/27-outro.mp4"
    output_path: "output/final.mp4"
    reencode: true

  # ========== Quality Audit ==========
  - id: audit_output
    kind: llm_audit
    assets:
      - "output/final.mp4"
    audit_prompt: |
      Analyze this frame for quality issues:
      - Is the image clear and not corrupted?
      - Are there any visual artifacts or glitches?
      Describe any issues found. Say "OK" if the frame looks good.
    output_path: "work/reports/audit.json"
    model: llava
    frame_count: 5
    fail_on_issues: false
    server: "{{ollama_server}}"
